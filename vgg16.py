# -*- coding: utf-8 -*-
"""Boke_VGGMTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11jLhf747xSnHrBb24ArJyLpLVjIxLw6h
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import os
import tensorflow as tf
import cv2
from tensorflow import keras
from  matplotlib import pyplot as plt
import matplotlib.image as mpimg
import librosa
from sklearn import metrics
from datetime import datetime
from os import mkdir
import pytz
import skimage
from skimage import io
from sklearn.model_selection import train_test_split
# %matplotlib inline

from google.colab import drive
drive.mount("/content/drive")

v3path = "/content/drive/My Drive/Smart Home/Dataset Collection/EkinNRTakes/"
images_path = "/content/drive/My Drive/Smart Home/Dataset Collection/DatasetV3Images"
gestures = ["Tap", "Swipe Right-Left", "Swipe Right", "Scroll Down", "Circle"]
dirs = os.listdir(v3path)
enum = {g:gestures.index(g) for g in gestures}

HOP_LENGTH = 512
N_MELS = 128
TIME_STEPS = 512 
IMAGE_X = 0
IMAGE_Y = N_MELS

def scale_minmax(X, min=0.0, max=1.0):
    X_std = (X - X.min()) / (X.max() - X.min())
    X_scaled = X_std * (max - min) + min
    return X_scaled

def spectrogram_image(y, sr, out, hop_length, n_mels):
    # use log-melspectrogram
    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels,
                                            n_fft=hop_length*2, hop_length=hop_length)
    mels = np.log(mels + 1e-9) # add small number to avoid log(0)

    # min-max scale to fit inside 8-bit range
    img = scale_minmax(mels, 0, 255).astype(np.uint8)
    img = np.flip(img, axis=0) # put low frequencies at the bottom in image
    img = 255-img # invert. make black==more energy

    IMAGE_X = img.shape[1]
    IMAGE_Y = img.shape[0]

    # save as PNG
    skimage.io.imsave(out, img)
    #fig, ax = plt.subplots()
    #im = ax.imshow(img)

# this part generates new timestamped folders for recordings
now = datetime.now()
timezone = pytz.timezone('Turkey')
now = now.astimezone(timezone)
dt_string = now.strftime("%d-%m-%Y %H:%M:%S")
count = 0
for f in os.listdir(images_path):
    count = max(count, int(f[0]))
new_dir = images_path + "/" + str(count+1) + " - Created at " + dt_string
os.mkdir(new_dir)
for g in gestures:
  os.mkdir(new_dir + "/" + g)

os.chdir(new_dir)

for dir in dirs:
  folder_path = v3path + "/" + dir
  recordings = os.listdir(folder_path)
  for recording in recordings:
    rec_path = folder_path + "/" + recording

    # settings
    hop_length = HOP_LENGTH # number of samples per time-step in spectrogram
    n_mels = N_MELS # number of bins in spectrogram. Height of image
    time_steps = TIME_STEPS # number of time-steps. Width of image
    # Keep halving the values until it works!


    # load audio.
    y, sr = librosa.load(rec_path, offset=0.5, duration=10.0, sr=44100)

    out_dir = os.getcwd() + "/" + dir
    if not os.path.exists(out_dir):
      os.makedirs(out_dir)    
    out = out_dir + "/" + recording[:-4] + ".png"

    # extract a fixed length window
    start_sample = 0 # starting at beginning
    length_samples = time_steps*hop_length
    window = y[start_sample:start_sample+length_samples]

    # convert to PNG
    spectrogram_image(window, sr=sr, out=out, hop_length=hop_length, n_mels=n_mels)
    print('wrote file', out)

import pandas as pd
import numpy as np
import os
import tensorflow as tf
import cv2
from tensorflow import keras
from  matplotlib import pyplot as plt
import matplotlib.image as mpimg
import librosa
from sklearn import metrics
from datetime import datetime
from os import mkdir
import pytz
import skimage
from skimage import io
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from tensorflow.keras import layers
from keras.utils import to_categorical
plt.style.use('fivethirtyeight')
import imageio
import glob

v3path = "/content/drive/My Drive/Smart Home/Dataset Collection/EkinNRTakes/"
images_path = "/content/drive/My Drive/Smart Home/Dataset Collection/DatasetV3Images"
gestures = ["Tap", "Swipe Right-Left", "Swipe Right", "Scroll Down", "Circle"]

# get the directory with the biggest number (I assume it is the most accurate)
last_dir = os.path.join(images_path, (sorted(os.listdir(images_path))[-1]))
print(last_dir)
os.chdir(last_dir)

images_path = os.getcwd()

img_dirs = os.listdir(images_path)
img_dirs

!pip install pillow

batch_size = 10 
img_height = 128
img_width = 44
# It should have exactly 3 input channels, and width and height should be no smaller than 32.

from keras.applications.vgg16 import VGG16, preprocess_input
from keras.applications import vgg16

# Create training dataset.
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
        images_path,
        validation_split=0.2,
        subset="training",
        seed=13,
        #color_mode = 'grayscale',
        color_mode = 'rgb',
        image_size=(img_height, img_width),
        batch_size=batch_size,
        shuffle=True,
        )

print(train_ds)

#Note: each Keras Application expects a specific kind of input preprocessing. For VGG16, call tf.keras.applications.vgg16.preprocess_input on your inputs before passing them to the model.

# Create validation dataset.
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  images_path,
  validation_split=0.2,
  subset="validation",
  seed=12,
  color_mode = 'rgb',
  image_size=(img_height, img_width),
  shuffle=True,
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

# Show the first 9 instances of the training set.
plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  #for i in range(9):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8").reshape(img_height, img_width,3))
    plt.title(class_names[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

# This is for easy IO.
AUTOTUNE = tf.data.experimental.AUTOTUNE

train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

num_classes = 5
input_shape = (128,44,3)

model = VGG16(weights="imagenet",include_top=False, input_shape = input_shape)
model = tf.keras.Model(inputs=model.inputs, outputs=model.layers[-1].output)
model.trainable = False

def pretrained_model(img_shape, num_classes):
    model_vgg16_conv = VGG16(weights='imagenet', include_top=False)
    #model_vgg16_conv.summary()
    
    #Create your own input format
    keras_input = Input(shape=img_shape, name = 'image_input')
    
    #Use the generated model 
    output_vgg16_conv = model_vgg16_conv(keras_input)
    
    #Add the fully-connected layers 
    x = Flatten(name='flatten')(output_vgg16_conv)
    x = Dense(4096, activation=layer_type, name='fc1')(x)
    x = Dense(4096, activation=layer_type, name='fc2')(x)
    x = Dense(num_classes, activation='softmax', name='predictions')(x)
    
    #Create your own model 
    pretrained_model = Model(inputs=keras_input, outputs=x)
    pretrained_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    return pretrained_model

flat1 = Flatten()(model.layers[-1].output)
#class1 = Dense(256, activation='relu')(flat1)
#dropout1 = Dropout(0.4)(class1)
#class2 = Dense(256, activation='relu')(dropout1)
#dropout2 = Dropout(0.4)(class2)
#class3 = Dense(256, activation='relu')(dropout2)
#output = Dense(num_classes, activation='softmax')(class3)
#model = tf.keras.Model(inputs=model.inputs, outputs=output)
class1 = Dense(512, activation='relu')(flat1)
dropout1 = Dropout(0.5)(class1)
class2 = Dense(512, activation='relu')(dropout1)
output = Dense(num_classes, activation='softmax')(class2)
model = tf.keras.Model(inputs=model.inputs, outputs=output)


for layer in model.layers:
  print(layer.trainable)

print(model.summary())

model.compile(
  optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
  #optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),
  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), 
  metrics=['accuracy'])

history = model.fit( train_ds, validation_data=val_ds, epochs=50)

loss_train = history.history['accuracy']
loss_val = history.history['val_accuracy']
epochs = range(1,51)
plt.plot(epochs, loss_train, 'g', label='Training accuracy')
plt.plot(epochs, loss_val, 'b', label='validation accuracy')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#visualize weights
#!pip install https://github.com/raghakot/keras-vis/archive/master.zip

model.save('/content/drive/My Drive/Smart Home/BOKE2_VGG-MODEL.h5')
